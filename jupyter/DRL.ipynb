{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import huskarl as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_env = lambda: gym.make('CartPole-v0').unwrapped\n",
    "dummy_env = create_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /anaconda3/lib/python3.7/site-packages/huskarl/agent/dqn.py\n",
    "from tensorflow.keras.layers import Dense, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from huskarl.policy import EpsGreedy, Greedy\n",
    "from huskarl.core import Agent, HkException\n",
    "from huskarl.memory import ExperienceReplay\n",
    "\n",
    "\n",
    "class DQN(Agent):\n",
    "\t\"\"\"Deep Q-Learning Network\n",
    "\n",
    "\tBase implementation:\n",
    "\t\t\"Playing Atari with Deep Reinforcement Learning\" (Mnih et al., 2013)\n",
    "\n",
    "\tExtensions:\n",
    "\t\tMulti-step returns: \"Reinforcement Learning: An Introduction\" 2nd ed. (Sutton & Barto, 2018)\n",
    "\t\tDouble Q-Learning: \"Deep Reinforcement Learning with Double Q-learning\" (van Hasselt et al., 2015)\n",
    "\t\tDueling Q-Network: \"Dueling Network Architectures for Deep Reinforcement Learning\" (Wang et al., 2016)\n",
    "\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, model, actions, optimizer=None, policy=None, test_policy=None,\n",
    "\t\t\t\t memsize=10_000, target_update=10, gamma=0.99, batch_size=64, nsteps=1,\n",
    "\t\t\t\t enable_double_dqn=True, enable_dueling_network=False, dueling_type='avg'):\n",
    "\t\t\"\"\"\n",
    "\t\tTODO: Describe parameters\n",
    "\t\t\"\"\"\n",
    "\t\tself.actions = actions\n",
    "\t\tself.optimizer = Adam(lr=3e-3) if optimizer is None else optimizer\n",
    "\n",
    "\t\tself.policy = EpsGreedy(0.1) if policy is None else policy\n",
    "\t\tself.test_policy = Greedy() if test_policy is None else test_policy\n",
    "\n",
    "\t\tself.memsize = memsize\n",
    "\t\tself.memory = ExperienceReplay(memsize, nsteps)\n",
    "\n",
    "\t\tself.target_update = target_update\n",
    "\t\tself.gamma = gamma\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.nsteps = nsteps\n",
    "\t\tself.training = True\n",
    "\n",
    "\t\t# Extension options\n",
    "\t\tself.enable_double_dqn = enable_double_dqn\n",
    "\t\tself.enable_dueling_network = enable_dueling_network\n",
    "\t\tself.dueling_type = dueling_type\n",
    "\n",
    "\t\t# Create output layer based on number of actions and (optionally) a dueling architecture\n",
    "\t\traw_output = model.layers[-1].output\n",
    "\t\tif self.enable_dueling_network:\n",
    "\t\t\t# \"Dueling Network Architectures for Deep Reinforcement Learning\" (Wang et al., 2016)\n",
    "\t\t\t# Output the state value (V) and the action-specific advantages (A) separately then compute the Q values: Q = A + V\n",
    "\t\t\tdueling_layer = Dense(self.actions + 1, activation='linear')(raw_output)\n",
    "\t\t\tif   self.dueling_type == 'avg':   f = lambda a: tf.expand_dims(a[:,0], -1) + a[:,1:] - tf.reduce_mean(a[:,1:], axis=1, keepdims=True)\n",
    "\t\t\telif self.dueling_type == 'max':   f = lambda a: tf.expand_dims(a[:,0], -1) + a[:,1:] - tf.reduce_max(a[:,1:], axis=1, keepdims=True)\n",
    "\t\t\telif self.dueling_type == 'naive': f = lambda a: tf.expand_dims(a[:,0], -1) + a[:,1:]\n",
    "\t\t\telse: raise HkException(\"dueling_type must be one of {'avg','max','naive'}\")\n",
    "\t\t\toutput_layer = Lambda(f, output_shape=(self.actions,))(dueling_layer)\n",
    "\t\telse:\n",
    "\t\t\toutput_layer = Dense(self.actions, activation='linear')(raw_output)\n",
    "\n",
    "\t\tself.model = Model(inputs=model.input, outputs=output_layer)\n",
    "\n",
    "\t\tdef masked_q_loss(data, y_pred):\n",
    "\t\t\t# Compute the MSE between the Q-values of the actions that were taken and\n",
    "\t\t\t# the cumulutive discounted rewards obtained after taking those actions\n",
    "\t\t\taction_batch, target_qvals = data[:,0], data[:,1]\n",
    "\t\t\tseq = tf.cast(tf.range(0, tf.shape(action_batch)[0]), tf.int32)\n",
    "\t\t\taction_idxs = tf.transpose(tf.stack([seq, tf.cast(action_batch, tf.int32)]))\n",
    "\t\t\tqvals = tf.gather_nd(y_pred, action_idxs)\n",
    "\t\t\treturn tf.keras.losses.mse(qvals, target_qvals)\n",
    "\n",
    "\t\tself.model.compile(optimizer=self.optimizer, loss=masked_q_loss)\n",
    "\n",
    "\t\t# Clone model to use for delayed Q targets\n",
    "\t\tself.target_model = tf.keras.models.clone_model(self.model)\n",
    "\t\tself.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "\tdef save(self, filename, overwrite=False):\n",
    "\t\t\"\"\"Saves the model parameters to the specified file.\"\"\"\n",
    "\t\tself.model.save_weights(filename, overwrite=overwrite)\n",
    "\n",
    "\tdef act(self, state, instance=0):\n",
    "\t\t\"\"\"Returns the action to be taken given a state.\"\"\"\n",
    "\t\tqvals = self.model.predict(np.array([state]))[0]\n",
    "\t\tif self.training:\n",
    "\t\t\treturn self.policy[instance].act(qvals) if isinstance(self.policy, list) else self.policy.act(qvals)\n",
    "\t\telse:\n",
    "\t\t\treturn self.test_policy[instance].act(qvals) if isinstance(self.test_policy, list) else self.test_policy.act(qvals)\n",
    "\n",
    "\tdef push(self, transition, instance=0):\n",
    "\t\t\"\"\"Stores the transition in memory.\"\"\"\n",
    "\t\tself.memory.put(transition, instance)\n",
    "\n",
    "\tdef train(self, step):\n",
    "\t\t\"\"\"Trains the agent for one step.\"\"\"\n",
    "\t\tif len(self.memory) == 0:\n",
    "\t\t\treturn\n",
    "\n",
    "\t\t# Update target network\n",
    "\t\tif self.target_update >= 1 and step % self.target_update == 0:\n",
    "\t\t\t# Perform a hard update\n",
    "\t\t\tself.target_model.set_weights(self.model.get_weights())\n",
    "\t\telif self.target_update < 1:\n",
    "\t\t\t# Perform a soft update\n",
    "\t\t\tmw = np.array(self.model.get_weights())\n",
    "\t\t\ttmw = np.array(self.target_model.get_weights())\n",
    "\t\t\tself.target_model.set_weights(self.target_update * mw + (1 - self.target_update) * tmw)\n",
    "\n",
    "\t\t# Train even when memory has fewer than the specified batch_size\n",
    "\t\tbatch_size = min(len(self.memory), self.batch_size)\n",
    "\n",
    "\t\t# Sample from memory (experience replay)\n",
    "\t\tstate_batch, action_batch, reward_batches, end_state_batch, not_done_mask = self.memory.get(batch_size)\n",
    "\n",
    "\t\t# Compute the value of the last next states\n",
    "\t\ttarget_qvals = np.zeros(batch_size)\n",
    "\t\tnon_final_last_next_states = [es for es in end_state_batch if es is not None]\n",
    "\n",
    "\t\tif len(non_final_last_next_states) > 0:\t\t\n",
    "\t\t\tif self.enable_double_dqn:\n",
    "\t\t\t\t# \"Deep Reinforcement Learning with Double Q-learning\" (van Hasselt et al., 2015)\n",
    "\t\t\t\t# The online network predicts the actions while the target network is used to estimate the Q-values\n",
    "\t\t\t\tq_values = self.model.predict_on_batch(np.array(non_final_last_next_states))\n",
    "\t\t\t\tactions = np.argmax(q_values, axis=1)\n",
    "\t\t\t\t# Estimate Q-values using the target network but select the values with the\n",
    "\t\t\t\t# highest Q-value wrt to the online model (as computed above).\n",
    "\t\t\t\ttarget_q_values = self.target_model.predict_on_batch(np.array(non_final_last_next_states))\n",
    "\t\t\t\tselected_target_q_vals = target_q_values[range(len(target_q_values)), actions]\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Use delayed target network to compute target Q-values\n",
    "\t\t\t\tselected_target_q_vals = self.target_model.predict_on_batch(np.array(non_final_last_next_states)).max(1)\n",
    "\t\t\tnon_final_mask = list(map(lambda s: s is not None, end_state_batch))\n",
    "\t\t\ttarget_qvals[non_final_mask] = selected_target_q_vals\n",
    "\n",
    "\t\t# Compute n-step discounted return\n",
    "\t\t# If episode ended within any sampled nstep trace - zero out remaining rewards\n",
    "\t\tfor n in reversed(range(self.nsteps)):\n",
    "\t\t\trewards = np.array([b[n] for b in reward_batches])\n",
    "\t\t\ttarget_qvals *= np.array([t[n] for t in not_done_mask])\n",
    "\t\t\ttarget_qvals = rewards + (self.gamma * target_qvals)\n",
    "\n",
    "\t\tloss_data = np.stack([action_batch, target_qvals]).transpose()\n",
    "\t\tself.model.train_on_batch(np.array(state_batch), loss_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
